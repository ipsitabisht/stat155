---
title: "project4_simulation_bisht_ipsita"
format: html
editor: visual
---

## Monte Carlo Simulation on Spotify Tracks

For this experiment, I would like to take a look at how adjusting the presence of hits in a dataset can affect our model (Artificial Neural Network) and its performance in predicting hits. All parts of the simulation haven't been fully implemented yet aside from the data generation step.

## Set up

```{python}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score, accuracy_score,  precision_score, recall_score, roc_auc_score 



from imblearn.over_sampling import RandomOverSampler
# loads our dataset and converts to dataframe
df = pd.read_csv("../Project_1/data/spotify_cleaned.csv")
```

## Data Generation Function

```{python}
def generate_data(n, hit_proportion, numeric_features, genres):
  data = {}
  for feature, (min_val, max_val) in numeric_feature_ranges.items():
      data[feature] = np.random.uniform(min_val, max_val, n)

  data['track_genre'] = np.random.choice(genres, n)

  df = pd.DataFrame(data)

  # get the number of hits based on the proportion specified in parameter
  num_hits = int(n * hit_proportion)
  is_hit = np.zeros(n, dtype=int)
  
  # assign the hit values randomly 
  hit_indices = np.random.choice(n, num_hits, replace=False)
  is_hit[hit_indices] = 1
  
  df['is_hit'] = is_hit
  
  return df


```

## Simulation Loop

```{python}

results = []
hit_proportions = [0.01, 0.2, 0.55]

numeric_features = ['danceability', 'energy', 'valence', 'acousticness', 'speechiness', 'instrumentalness', 'tempo', 'loudness']
numeric_feature_ranges = {
    col: (df[col].min(), df[col].max()) for col in numeric_features
}

distinct_genres = ['jazz', 'country', 'rock', 'dubstep', 'pop', 'heavy-metal','bluegrass', 'soul', 'reggaeton', 'house', 'techno', 'k-pop']

# Define models to compare
models = {
    "ANN": MLPClassifier(
        hidden_layer_sizes=(64, 32),
        activation='relu',
        solver='adam',
        batch_size=64,
        learning_rate_init=0.001,
        max_iter=500,
        random_state=42,
        early_stopping=True,
        n_iter_no_change=20
    ),
    "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),
}
# loop through hit levels
for prop in hit_proportions:
    for i in range(20):
      # generate our dataset 
      df_gen = generate_data(10000, prop, numeric_feature_ranges, distinct_genres)
      X = df_gen.drop('is_hit', axis=1)
      y = df_gen['is_hit']
      
      # split into training and testing sets
      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)
      
      # preprocess and onehotencode
      
      scaler = StandardScaler()
      X_train_scaled = scaler.fit_transform(X_train[numeric_features])
      X_test_scaled = scaler.transform(X_test[numeric_features])
      
      ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
      encoded_train = ohe.fit_transform(X_train[['track_genre']])
      encoded_test = ohe.transform(X_test[['track_genre']])
      
      genre_cols = ohe.get_feature_names_out(['track_genre'])
      
      X_train_processed = pd.DataFrame(X_train_scaled, columns=numeric_features, index=X_train.index)
      X_train_processed = pd.concat([X_train_processed, pd.DataFrame(encoded_train, columns=genre_cols, index=X_train.index)], axis=1)
      
      X_test_processed = pd.DataFrame(X_test_scaled, columns=numeric_features, index=X_test.index)
      X_test_processed = pd.concat([X_test_processed, pd.DataFrame(encoded_test, columns=genre_cols, index=X_test.index)], axis=1)
      
      ros = RandomOverSampler(random_state=42)
      X_resampled, y_resampled = ros.fit_resample(X_train_processed, y_train)
      
    
      # test with two models 
      for model_name, model in models.items():
            model.fit(X_train_processed, y_train)
            y_pred = model.predict(X_test_processed)
            cm = confusion_matrix(y_test, y_pred)
            print(f"\nConfusion Matrix for {model_name} | Hit Prevalence: {prop}, Run: {i + 1}")
            print(cm)
            
            # calculate metrics 
            accuracy = accuracy_score(y_test, y_pred)
            f1 = round(f1_score(y_test, y_pred, average='binary', pos_label=1),10)
            precision = round(precision_score(y_test, y_pred, average='binary',pos_label=1, zero_division=0),10)
            recall = round(recall_score(y_test, y_pred, average='binary', pos_label=1, zero_division=0),10)
            
           
            results.append({
                    'Model': model_name,
                    'Prevalence': prop,
                    'Simulation_Run': i + 1,
                    'Accuracy': accuracy,
                    'Precision': precision,
                    'Recall': recall,
                    'F1_Score': f1
                })
      
```

## Results Summary

```{python}
results_df = pd.DataFrame(results)
summary = results_df.groupby(['Model', 'Prevalence']).agg(
    Mean_Accuracy=('Accuracy', 'mean'),
    SD_Accuracy=('Accuracy', 'std'),
    Mean_F1_Score=('F1_Score', 'mean'),
    SD_F1_Score=('F1_Score', 'std'),
    Mean_Precision=('Precision', 'mean'),
    SD_Precision=('Precision', 'std'),
    Mean_Recall=('Recall', 'mean'),
    SD_Recall=('Recall', 'std'),  
).reset_index()

summary
```

```{python}

#ANOVA ANALYSIS
from statsmodels.formula.api import ols
import statsmodels.api as sm

model = ols('F1_Score ~ C(Model) * C(Prevalence)', data=results_df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

```

```{python}
plt.figure(figsize=(10, 6))
sns.boxplot(data=results_df, x='Prevalence', y='F1_Score', hue='Model')
plt.title("Model Performance (F1 Score) by Hit Prevalence")
plt.xlabel("Hit Prevalence")
plt.ylabel("F1 Score")
plt.legend(title="Model")
plt.tight_layout()
plt.show()
```

\<style scoped\> .dataframe tbody tr th:only-of-type { vertical-align: middle; }

```         
.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
```

\</style\>
